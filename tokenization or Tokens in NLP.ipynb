{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ebe51b",
   "metadata": {},
   "source": [
    "# Word Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e6dc1",
   "metadata": {},
   "source": [
    "Tokenization is essentially splitting a phrase, sentence, paragraph, \n",
    "or an entire text document into smaller units, such as individual words or terms. \n",
    "Each of these smaller units are called tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba4bf5",
   "metadata": {},
   "source": [
    "# Word Tokenization no: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5f60513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded',\n",
       " 'in',\n",
       " '2002,',\n",
       " 'SpaceX’s',\n",
       " 'mission',\n",
       " 'spacefaring',\n",
       " 'civilization.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "\n",
    "text = \"\"\"Founded in 2002, SpaceX’s mission spacefaring civilization.\"\"\"\n",
    "\n",
    "# Splits at space \n",
    "text.split() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8d940",
   "metadata": {},
   "source": [
    "# Word Tokenization no: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd85114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3eb18d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['species',\n",
       " 'by',\n",
       " 'building',\n",
       " 'the',\n",
       " 'first',\n",
       " 'privately',\n",
       " 'developed',\n",
       " 'liquid',\n",
       " 'fuel',\n",
       " 'launch',\n",
       " 'vehicle',\n",
       " 'to',\n",
       " 'orbit',\n",
       " 'the',\n",
       " 'earth']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = \"\"\"\n",
    "species by building  the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "\n",
    "# tokenize\n",
    "result = text_to_word_sequence(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658bf584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaf1bc8c",
   "metadata": {},
   "source": [
    "# Tokenize no 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64618d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Create text\n",
    "\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "# Tokenize words\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2439d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc0931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5aa7fae",
   "metadata": {},
   "source": [
    "# Sentence Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871463f",
   "metadata": {},
   "source": [
    "# Sentence Tokenization no 1\n",
    "\n",
    "This is similar to word tokenization. Here, we study the structure of sentences in the analysis. A sentence usually ends with a full stop (.), so we can use “.” as a separator to break the string:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efffba51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded in 2002 civilization and a multi-planet',\n",
       " 'Species by building a self-sustaining\\ncity on Mars',\n",
       " 'The first privately developed \\nliquid-fuel launch vehicle to orbit the Earth.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "\n",
    "text = \"\"\"Founded in 2002 civilization and a multi-planet. Species by building a self-sustaining\n",
    "city on Mars. The first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "\n",
    "# Splits at '.' \n",
    "text.split('. ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fdac1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b83e79ca",
   "metadata": {},
   "source": [
    "# Tokenize no 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e72b28be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.',\n",
       " 'Tomorrow is today.',\n",
       " 'Today is very impotant for me.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Create text\n",
    "\n",
    "string = \"\"\"The science of today is the technology of tomorrow. \n",
    "Tomorrow is today. Today is very impotant for me.\"\"\"\n",
    "\n",
    "# Tokenize sentences\n",
    "sent_tokenize(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4c3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82f68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000265d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba06dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce9fb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'like')\n",
      "('like', 'dancing')\n",
      "('dancing', 'in')\n",
      "('in', 'the')\n",
      "('the', 'rain')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    " \n",
    "sentence = 'I like dancing in the rain'\n",
    " \n",
    "ngram = ngrams(sentence.split(' '), n=2)\n",
    " \n",
    "for x in ngram:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383bf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03034b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94406c06",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826c34f",
   "metadata": {},
   "source": [
    "Term Frequency, Inverse Document Frequency(TF-IDF):\n",
    "-\n",
    "This is the most popular way to represent documents as feature vectors. TF-IDF stands for Term Frequency, Inverse Document Frequency.\n",
    "\n",
    "TF-IDF measures how important a particular word is with respect to a document and the entire corpus.\n",
    "\n",
    "Term Frequency:\n",
    "\n",
    "Term frequency is the measure of the counts of each word in a document out of all the words in the same document. \n",
    "\n",
    "TF(w) = (number of times word w appears in a document) / (total number of words in the document)\n",
    "\n",
    "For example, if we want to find the TF of the word cat which occurs 50 times in a document of 1000 words, then \n",
    "\n",
    "TF(cat) = 50 / 1000 = 0.05\n",
    "\n",
    "Inverse Document Frequency:\n",
    "\n",
    "IDF is a measure of the importance of a word, taking into consideration the frequency of the word throughout the corpus.\n",
    "\n",
    "It measures how important a word is for the corpus.\n",
    "\n",
    "IDF(w) = log(total number of documents / number of documents with w in it)\n",
    "\n",
    "For example, if the word cat occurs in 100 documents out of 3000, then the IDF is calculated as\n",
    "\n",
    "IDF(cat) = log(3000 / 100) = 1.47\n",
    "\n",
    "Finally, to calculate TF-IDF, we multiply these two factors – TF and IDF.\n",
    "\n",
    "TF-IDF(w) = TF(w) x IDF(w)\n",
    "\n",
    "TF-IDF(cat) = 0.05 * 1.47 = 0.073\n",
    "\n",
    "Let’s do some coding.\n",
    "\n",
    "We’ll use the TfidfVectorizer from scikit-learn for vectorizing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c5a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60a2e707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary       and  antagonistic       are      cats      dogs      four  \\\n",
      "sentence1   0.000000      0.000000  0.000000  0.402040  0.000000  0.528635   \n",
      "sentence2   0.490479      0.490479  0.490479  0.373022  0.373022  0.000000   \n",
      "sentence3   0.000000      0.000000  0.000000  0.000000  0.473630  0.000000   \n",
      "\n",
      "vocabulary      hate      have        he      legs  \n",
      "sentence1   0.000000  0.528635  0.000000  0.528635  \n",
      "sentence2   0.000000  0.000000  0.000000  0.000000  \n",
      "sentence3   0.622766  0.000000  0.622766  0.000000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = ['Cats have four legs',\n",
    "          'Cats and dogs are antagonistic',\n",
    "          'He hate dogs']\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "vect = tfidf.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['vocabulary'] = tfidf.get_feature_names_out()\n",
    "df['sentence1'] = vect.toarray()[0]\n",
    "df['sentence2'] = vect.toarray()[1]\n",
    "df['sentence3'] = vect.toarray()[2]\n",
    "df.set_index('vocabulary', inplace=True)\n",
    "print(df.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83a14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1c747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb2bf83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary       and  antagonistic       are      cats      dogs      four  \\\n",
      "sentence1   0.000000      0.000000  0.000000  0.402040  0.000000  0.528635   \n",
      "sentence2   0.490479      0.490479  0.490479  0.373022  0.373022  0.000000   \n",
      "sentence3   0.000000      0.000000  0.000000  0.000000  0.473630  0.000000   \n",
      "\n",
      "vocabulary      hate      have        he      legs  \n",
      "sentence1   0.000000  0.528635  0.000000  0.528635  \n",
      "sentence2   0.000000  0.000000  0.000000  0.000000  \n",
      "sentence3   0.622766  0.000000  0.622766  0.000000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    " \n",
    "corpus = ['Cats have four legs',\n",
    "          'Cats and dogs are antagonistic',\n",
    "          'He hate dogs']\n",
    " \n",
    "tfidf = TfidfVectorizer()\n",
    "vect = tfidf.fit_transform(corpus)\n",
    " \n",
    "df = pd.DataFrame()\n",
    "df['vocabulary'] = tfidf.get_feature_names_out ()\n",
    "df['sentence1'] = vect.toarray()[0]\n",
    "df['sentence2'] = vect.toarray()[1]\n",
    "df['sentence3'] = vect.toarray()[2]\n",
    "df.set_index('vocabulary', inplace=True)\n",
    "print(df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a4598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
